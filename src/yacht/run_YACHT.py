#!/usr/bin/env python
import os
import sys
import pandas as pd
from pathlib import Path
from . import hypothesis_recovery_src as hr
import argparse
from . import utils
import json
import warnings
import zipfile
import glob
from loguru import logger
from .utils import decompress_all_sig_files
warnings.filterwarnings("ignore")

# Configure Loguru logger
logger.remove()
logger.add(
    sys.stdout, format="{time:YYYY-MM-DD HH:mm:ss} - {level} - {message}", level="INFO"
)


def add_arguments(parser):
    parser.add_argument(
        "--json",
        type=str,
        help="Path to a json file generated by make_training_data_from_sketches.py.",
        required=True,
    )
    parser.add_argument(
        "--sample_file", help="Metagenomic sample in .sig.zip format", required=True
    )
    parser.add_argument(
        "--significance",
        type=float,
        help="Minimum probability of individual true negative.",
        required=False,
        default=0.99,
    )
    parser.add_argument(
        "--num_threads",
        type=int,
        help="Number of threads to use for parallelization.",
        required=False,
        default=16,
    )
    parser.add_argument(
        "--keep_raw", action="store_true", help="Keep raw results in output file."
    )
    parser.add_argument(
        "--show_all",
        action="store_true",
        help="Show all organisms (no matter if present) in output file.",
    )
    parser.add_argument(
        "--min_coverage_list",
        nargs="+",
        type=float,
        help="A list of percentages of unique k-mers covered by reads in the sample. "
        "Each value should be between 0 and 1, with 0 being the most sensitive (and least "
        "precise) and 1 being the most precise (and least sensitive).",
        required=False,
        default=[1, 0.5, 0.1, 0.05, 0.01],
    )
    parser.add_argument(
        "--outdir",
        type=str,
        help="Path to output location where the 'results' folder will be created.",
        required=False,
        default=os.getcwd(),
    )


def main(args):
    json_file_path = str(Path(args.json).absolute())  # path to json file
    sample_file = str(Path(args.sample_file).absolute())  # location of sample.sig file
    significance = args.significance  # Minimum probability of individual true negative.
    num_threads = args.num_threads  # Number of threads to use for parallelization.
    keep_raw = args.keep_raw  # Keep raw results in output file.
    show_all = args.show_all # Show all organisms (no matter if present) in output file.
    min_coverage_list = args.min_coverage_list # a list of percentages of unique k-mers covered by reads in the sample.
    outdir = str(Path(args.outdir).absolute())  # path to output location
    
    # Define fixed results folder and output filenames
    results_folder = os.path.join(outdir, "results")
    raw_result_txt_file = os.path.join(results_folder, "result_all.txt")
    excel_result_file = os.path.join(results_folder, "result.xlsx")
    
    # Create results folder if it doesn't exist
    if not os.path.exists(results_folder):
        logger.info(f"Creating results folder: {results_folder}")
        os.makedirs(results_folder, exist_ok=True)

    # check if the json file exists
    utils.check_file_existence(
        json_file_path,
        f"Config file {json_file_path} does not exist. "
        f"Please run make_training_data_from_sketches.py first.",
    )
    # load the config file, ksize, and ani_thresh
    config = json.load(open(json_file_path, "r"))
    manifest_file_path = config["manifest_file_path"]
    path_to_genome_temp_dir = config["intermediate_files_dir"]
    scale = config["scale"]
    ksize = config["ksize"]
    ani_thresh = config["ani_thresh"]

    # Make sure the output location can be written to
    if not os.path.exists(outdir):
        print(f"Output location does not exist: {outdir}.\n")
        print("Please check if this location exists. Exiting..\n")
        sys.exit(1)
    if not os.access(outdir, os.W_OK):
        # give error message, and exit with error status
        print(f"Cannot write to the location: {outdir}.\n")
        print("Please check that you have the permission to write to this location. Exiting..\n")
        sys.exit(1)

    # check if min_coverage is between 0 and 1
    for x in min_coverage_list:
        if not (0 <= x <= 1):
            raise ValueError(
                f"One of values in the min_coverage_list you provided {x} is not between 0 and 1. Please check your input."
            )

    # make sure all these files exist
    utils.check_file_existence(
        manifest_file_path,
        f"The manifest file {manifest_file_path} "
        f"does not exist. Please check if you are using the correct json file as input.",
    )

    # load the training data
    logger.info("Loading the manifest file generated from the training data.")
    manifest = pd.read_csv(manifest_file_path, sep="\t", header=0)

    # check if there is a manifest in the sample sig file
    with zipfile.ZipFile(sample_file, "r") as zip_file:
        if "SOURMASH-MANIFEST.csv" not in zip_file.namelist():
            raise FileNotFoundError(
                f"The input file {sample_file} appears to be missing a manifest associated with it. "
                f"Try running: sourmash sig merge {sample_file} -o <new signature with the manifest present>. "
                f"And then run YACHT using the output of that command."
            )

    # load sample signature and its signature info
    logger.info("Loading sample signature and its signature info.")
    try:
        sample_sig = utils.load_signature_with_ksize(sample_file, ksize)
    except ValueError:
        raise ValueError(
            f"Expected exactly one signature with ksize {ksize} in {sample_file}, found {len(sample_file)}. "
            f"Likely you will need to do something like: sourmash sig merge {sample_file} -o <new signature with just one sketch in it>."
        )
    sample_sig_info = utils.get_info_from_single_sig(sample_file, ksize)

    # add sample signature info to the manifest
    manifest["num_exclusive_kmers_in_sample_sketch"] = sample_sig_info[3]
    manifest["num_total_kmers_in_sample_sketch"] = utils.get_num_kmers(
        sample_sig_info[3], sample_sig_info[4], sample_sig_info[5], scale=False
    )
    manifest["sample_scale_factor"] = sample_sig_info[5]
    manifest["min_coverage"] = 1.0

    # check that the sample scale factor is the same as the genome scale factor for all organisms
    if scale != sample_sig_info[5]:
        raise ValueError(
            "Sample scale factor does not equal genome scale factor. Please check your input."
        )

    # compute hypothesis recovery
    logger.info("Computing hypothesis recovery.")
    sample_info_set = (sample_file, sample_sig)
    min_coverage_list = list(set(min_coverage_list))
    min_coverage_list.sort(reverse=True)
    has_raw = False
    if 1.0 not in min_coverage_list:
        min_coverage_list = [1.0] + min_coverage_list
    else:
        has_raw = True

    # a patch to check if the genome signature files have been decompressed
    training_sig_file = glob.glob(f"{path_to_genome_temp_dir}/training_sig_files.*")[0]
    df = pd.read_csv(training_sig_file, sep="\t", header=None)
    if 'sig.gz' in df[0].values[0]:
        pd.DataFrame([x.replace('sig.gz','sig') for x in df[0]]).to_csv(training_sig_file, header=False, index=False)
        all_gz_files = glob.glob(f"{path_to_genome_temp_dir}/signatures/*.sig.gz")
        decompress_all_sig_files(all_gz_files, num_threads)  

    manifest_list = hr.hypothesis_recovery(
        manifest,
        sample_info_set,
        path_to_genome_temp_dir,
        min_coverage_list,
        scale,
        ksize,
        significance,
        ani_thresh,
        num_threads,
    )

    # remove unnecessary columns
    remove_cols = ["md5sum", "sample_scale_factor"]
    temp_manifest_list = []
    for temp_manifest in manifest_list:
        temp_manifest = temp_manifest[
            [col for col in temp_manifest.columns if col not in remove_cols]
        ]
        temp_manifest.rename(
            columns={"genome_scale_factor": "scale_factor"}, inplace=True
        )
        temp_manifest_list += [temp_manifest]
    manifest_list = temp_manifest_list

    # process the results and save them to output files
    logger.info(f"Saving results to {results_folder}.")
    
    # Save all unfiltered results for all user-given min_coverage values to a TXT file
    # Concatenate all manifest dataframes
    if has_raw:
        # User provided min_coverage=1.0, include all results
        all_manifests = pd.concat(manifest_list, ignore_index=True)
    else:
        # min_coverage=1.0 was added internally, exclude it (first element)
        all_manifests = pd.concat(manifest_list[1:], ignore_index=True)
    all_manifests.to_csv(raw_result_txt_file, sep="\t", index=False)
    logger.info(f"Unfiltered results for all min_coverage values saved to {raw_result_txt_file}")
    
    # save the results with different min_coverage to Excel file
    with pd.ExcelWriter(excel_result_file, engine="openpyxl", mode="w") as writer:
        # save the raw results (i.e., min_coverage=1.0)
        if keep_raw:
            temp_mainifest = manifest_list[0].copy()
            temp_mainifest.rename(
                columns={
                    "acceptance_threshold_with_coverage": "acceptance_threshold_wo_coverage",
                    "actual_confidence_with_coverage": "actual_confidence_wo_coverage",
                    "alt_confidence_mut_rate_with_coverage": "alt_confidence_mut_rate_wo_coverage",
                },
                inplace=True,
            )
            temp_mainifest.to_excel(writer, sheet_name="raw_result", index=False)
        # save the results with different min_coverage given by the user
        if not has_raw:
            min_coverage_list = min_coverage_list[1:]
            manifest_list = manifest_list[1:]

        for min_coverage, temp_mainifest in zip(min_coverage_list, manifest_list):
            if not show_all:
                temp_mainifest = temp_mainifest[temp_mainifest["in_sample_est"] == True]
            temp_mainifest.to_excel(
                writer, sheet_name=f"min_coverage{min_coverage}", index=False
            )


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="This script estimates the abundance of microorganisms from a "
        "reference database matrix and metagenomic sample.",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    add_arguments(parser)
    args = parser.parse_args()
    main(args)
